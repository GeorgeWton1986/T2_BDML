---
title: "Taller_2"
author: GARCIA BERNAL, ZAIRA ALEJANDRA RIVERA SANABRIA, LAURA SARIF JACOME VELASCO,
  NICOLAS VIAFARA MORALES, JORGE ELIECER
date: "2025-03-22"
output: html_document
---

```{r}
#Cargar los paquetes de trabajo
require("pacman")
p_load(tidyverse, # tidy-data
       dplyr,
       glmnet, # To implement regularization algorithms. 
       caret, # creating predictive models
       BiocManager,
       Metrics,
       readr,
       utils,
       rpart,
       rpart.plot,
       xgboost,
       pROC,
       ggplot2,
       skimr)

install.packages("MLmetrics") # To calculate metrics
require(MLmetrics)
```

```{r}
## 1. Carga de datos train hogares

library(httr)

# Definir URL del archivo Excel en GitHub
url_excel <- "https://raw.githubusercontent.com/GeorgeWton1986/T2_BDML/refs/heads/main/03_Stores/train_hogares.csv"

# Descargar el archivo temporalmente
temp_file <- tempfile(fileext = ".csv")
GET(url_excel, write_disk(temp_file, overwrite = TRUE))

# Leer el archivo CSV en un dataframe
train_hogares <- read_csv(temp_file)

## 2. Carga de datos train personas

# URL del archivo ZIP en GitHub
url_zip <- "https://github.com/GeorgeWton1986/T2_BDML/raw/refs/heads/main/03_Stores/train_personas.zip"

# Crear un archivo temporal para almacenar el ZIP
temp_zip <- tempfile(fileext = ".zip")

# Descargar el archivo ZIP desde GitHub
GET(url_zip, write_disk(temp_zip, overwrite = TRUE))
temp_dir <- tempfile()
dir.create(temp_dir)

# Extraer el contenido del ZIP
unzip(temp_zip, exdir = temp_dir)

# Leer el archivo CSV extraído
csv_personas1 <- file.path(temp_dir, "train_personas.csv")
train_personas <- read_csv(csv_personas1)

## 3. Carga de datos test hogares

# Definir URL del archivo Excel en GitHub
url_excel_test <- "https://raw.githubusercontent.com/GeorgeWton1986/T2_BDML/refs/heads/main/03_Stores/test_hogares.csv"

# Descargar el archivo temporalmente
temp_file <- tempfile(fileext = ".csv")
GET(url_excel_test, write_disk(temp_file, overwrite = TRUE))

# Leer el archivo CSV en un dataframe
test_hogares <- read_csv(temp_file)

## 4. Carga de datos test personas

# Definir URL del archivo Excel en GitHub
url_excel_test2 <- "https://raw.githubusercontent.com/GeorgeWton1986/T2_BDML/refs/heads/main/03_Stores/test_personas.csv"

# Descargar el archivo temporalmente
temp_file <- tempfile(fileext = ".csv")
GET(url_excel_test2, write_disk(temp_file, overwrite = TRUE))

# Leer el archivo CSV en un dataframe
test_personas <- read_csv(temp_file)
```

##A.1 Inspección de Base Train Hogares

```{r}
#Nombre de las columnas de la base train_hogares
colnames(train_hogares)

#Seleccion de la columnas id de hogares
train_hogares %>%
  select(id) %>%
  head()

#Cantidad de hogares probres de la base train_hogares
table(train_hogares$Pobre)

#Incluir una columna con la asignacion de 1 si el ingreso per capita hogar es menor a la linea de probreza y 0 en caso contrario
train_hogares <- train_hogares %>% 
  mutate(pobre_hand=ifelse(Ingpcug<Lp,1,0))

#Representación en tabla de los resultado
table(train_hogares$Pobre,train_hogares$pobre_hand)

```

##A.2 Inspeccionar Base Train Personas

```{r}
#Nombre de las columnas de la base train_personas

colnames(train_personas)

#Selección de la columnas id y orden de los mienbros de la base personas
train_personas %>%
  select(id, Orden) %>%
  head()

#Agrupa las personas por hogar y calculo del indicie de personas inactivas
train_personas_1<- train_personas %>%
  group_by(id)%>%
  summarize(h_inactivos=sum(Ina, na.rm = TRUE),
            h_pet=sum(Pet, na.rm = TRUE))%>%
  mutate(h_inactivosp=h_inactivos/h_pet) %>%
  ungroup()
```

#B Test

##B.1 Inspección Base Test Hogares

```{r}
#Nombre de las columnas de la base test_hogares

colnames(test_hogares)

#Selección de la columnas id y número de personas por hogar
test_hogares %>%
  select(id, Npersug) %>%
  head()
```

##B.2 Inspección Base Test Personas

```{r}
#Nombre de las columnas de la base test_personas

colnames(test_personas)
```
## B.3 Estadísticas descriptivas

```{r}
skim(train)
skim(test)
```


#1. Pre procesamiento de Train personas y Train hogares

```{r}
#Pre procesamiento de la base Train_Personas
clear_train_personas <- train_personas %>% 
  mutate(mujer = ifelse(P6020==2,1,0),
  mayor_edad = ifelse(P6040 >= 18,1,0), #Mayores de edad
  H_Head = ifelse(P6050== 1, 1, 0), #Household head
  afi_salud = ifelse (P6090==1,1,0), #Afiliacion de salud
  afi_salud = ifelse (is.na(afi_salud),0,1), #Reemplazo de los NA
  EducLevel = ifelse(P6210==9,0,P6210), #Replace 9 with 0
  EducLevel = ifelse(is.na(EducLevel),0,EducLevel),#Reemplazo de los NA
  act_semana_pasada = ifelse(P6240 == 6,0,P6240),
  act_semana_pasada = ifelse(is.na(act_semana_pasada),0,act_semana_pasada), #Reemplazo de los NA
  t_trab_semanal = ifelse(is.na(P6426),0,1),
  cotizando = ifelse(P6920 == 3, 1, P6920), #Incluir los pensionados
  cotizando = ifelse(is.na(cotizando),0,cotizando),#Reemplazo de los NA
  ocupado = ifelse(is.na(Oc),0,1)) %>%
  select(id, Orden,mujer,mayor_edad,H_Head,afi_salud,EducLevel,act_semana_pasada,t_trab_semanal,cotizando,ocupado)

#Pre procesamiento de la base Test_Personas
clear_test_personas <- test_personas %>% 
  mutate(mujer = ifelse(P6020==2,1,0),
  mayor_edad = ifelse(P6040 >= 18,1,0), #Mayores de edad
  H_Head = ifelse(P6050== 1, 1, 0), #Household head
  afi_salud = ifelse (P6090==1,1,0), #Afiliacion de salud
  afi_salud = ifelse (is.na(afi_salud),0,1), #Reemplazo de los NA
  EducLevel = ifelse(P6210==9,0,P6210), #Replace 9 with 0
  EducLevel = ifelse(is.na(EducLevel),0,EducLevel),#Reemplazo de los NA
  act_semana_pasada = ifelse(P6240 == 6,0,P6240),
  act_semana_pasada = ifelse(is.na(act_semana_pasada),0,act_semana_pasada), #Reemplazo de los NA
  t_trab_semanal = ifelse(is.na(P6426),0,1),
  cotizando = ifelse(P6920 == 3, 1, P6920), #Incluir los pensionados
  cotizando = ifelse(is.na(cotizando),0,cotizando),#Reemplazo de los NA
  ocupado = ifelse(is.na(Oc),0,1)) %>%
  select(id, Orden,mujer,mayor_edad,H_Head,afi_salud,EducLevel,act_semana_pasada,t_trab_semanal,cotizando,ocupado)

colnames(clear_train_personas)

colnames(clear_test_personas)
```

##1.2 Agrupar y unir Train Personas

```{r}
#Agrupar el entrenamiento de la base personas a nivel hogar y unir las bases de datos por medio de id.

summary_train_personas_nivel_hogar <- clear_train_personas %>% 
  group_by(id) %>% 
  summarize(nmujeres = sum(mujer,na.rm=TRUE),
            nmayor_edad = sum(mayor_edad,na.rm=TRUE),
            nafi_salud = sum(afi_salud, na.rm = TRUE),
            maxEducLevel = max(EducLevel,na.rm=TRUE),
            nt_trab_semanal = sum(t_trab_semanal,na.rm = TRUE),
            ncotizando = sum(cotizando,na.rm = TRUE),
            nocupados=sum(ocupado,na.rm=TRUE))

filter_train_personas_hogar<- clear_train_personas %>% 
                  filter(H_Head==1) %>% 
                  select(id,mujer,afi_salud,EducLevel,cotizando,ocupado) %>% 
                  rename(H_Head_mujer=mujer,
                         H_afi_salud = afi_salud,
                         H_Head_Educ_level=EducLevel,
                         H_cotizando = cotizando,
                         H_Head_ocupado=ocupado) %>% 
                    left_join(summary_train_personas_nivel_hogar)

colnames(filter_train_personas_hogar)
```

##1.3 Agrupar y unir Test Personas

```{r}
#Agrupar la prueba de la base personas a nivel hogar y unir las bases de datos por medio de id.


summary_test_personas_nivel_hogar <- clear_test_personas %>% 
  group_by(id) %>% 
  summarize(nmujeres = sum(mujer,na.rm=TRUE),
            nmayor_edad = sum(mayor_edad,na.rm=TRUE),
            nafi_salud = sum(afi_salud, na.rm = TRUE),
            maxEducLevel = max(EducLevel,na.rm=TRUE),
            nt_trab_semanal = sum(t_trab_semanal,na.rm = TRUE),
            ncotizando = sum(cotizando,na.rm = TRUE),
            nocupados=sum(ocupado,na.rm=TRUE))

filter_test_personas_hogar<- clear_test_personas %>% 
                  filter(H_Head==1) %>% 
                  select(id,mujer,afi_salud,EducLevel,cotizando,ocupado) %>% 
                  rename(H_Head_mujer=mujer,
                         H_afi_salud = afi_salud,
                         H_Head_Educ_level=EducLevel,
                         H_cotizando = cotizando,
                         H_Head_ocupado=ocupado) %>% 
                    left_join(summary_test_personas_nivel_hogar, by = "id")

colnames(filter_test_personas_hogar)
```

##1.4 Base de trabajo Train Hogares

```{r}
clear_train_hogares<- train_hogares %>% 
  mutate(pagada= ifelse(P5090==1,1,0),
         hipoteca= ifelse(P5090==2,1,0),
         arriendo= ifelse(P5090==3,1,0),
         usufructo= ifelse(P5090==4,1,0),
         sin_titulo = ifelse(P5090==5,1,0)) %>% 
  select(id, Clase, Nper, Dominio,pagada, hipoteca, arriendo, usufructo, sin_titulo, Lp, Pobre)
```

##1.5 Base de trabajo Test Hogares

```{r}
clear_test_hogares<- test_hogares %>% 
  mutate(pagada= ifelse(P5090==1,1,0),
         hipoteca= ifelse(P5090==2,1,0),
         arriendo= ifelse(P5090==3,1,0),
         usufructo= ifelse(P5090==4,1,0),
         sin_titulo = ifelse(P5090==5,1,0)) %>% 
  select(id, Clase, Nper, Dominio, pagada, hipoteca, arriendo, usufructo, sin_titulo, Lp)
```

##1.6 Consolidar las BD Train y Test Hogares

```{r}
train<- clear_train_hogares %>% 
          left_join(filter_train_personas_hogar)%>% 
          select(-id) #no longer need id

test<- clear_test_hogares %>% 
          left_join(filter_test_personas_hogar)

```

##1.7 Conversion a factores

```{r}
train_factors<- train %>% 
  mutate(Dominio=factor(Dominio),
         pagada=factor(pagada,levels = c(0,1),labels = c("No","Yes")),
         hipoteca=factor(hipoteca,levels = c(0,1),labels = c("No","Yes")),
         arriendo=factor(arriendo,levels = c(0,1),labels = c("No","Yes")),
         usufructo=factor(usufructo,levels = c(0,1),labels = c("No","Yes")),
         sin_titulo=factor(sin_titulo,levels = c(0,1),labels = c("No","Yes")), #No se tiene documento de propiedad
         Pobre=factor(Pobre,levels = c(0,1),labels = c("No", "Yes")),
         H_Head_Educ_level = factor(H_Head_Educ_level, levels = c(0:6),labels = c("Ns",'Ninguno', 'Preescolar','Primaria', 'Secundaria','Media', 'Universitaria')),
         maxEducLevel = factor(maxEducLevel,levels = c(0:6),labels = c("Ns",'Ninguno', 'Preescolar','Primaria', 'Secundaria','Media', 'Universitaria') ))

test_factors<- test %>% 
  mutate(Dominio=factor(Dominio),
         pagada=factor(pagada,levels = c(0,1),labels = c("No","Yes")),
         hipoteca=factor(hipoteca,levels = c(0,1),labels = c("No","Yes")),
         arriendo=factor(arriendo,levels = c(0,1),labels = c("No","Yes")),
         usufructo=factor(usufructo,levels = c(0,1),labels = c("No","Yes")),
         sin_titulo=factor(sin_titulo,levels = c(0,1),labels = c("No","Yes")), #No se tiene documento de propiedad
         H_Head_Educ_level = factor(H_Head_Educ_level, levels = c(0:6),labels = c("Ns",'Ninguno', 'Preescolar','Primaria', 'Secundaria','Media', 'Universitaria')),
         maxEducLevel = factor(maxEducLevel,levels = c(0:6),labels = c("Ns",'Ninguno', 'Preescolar','Primaria', 'Secundaria','Media', 'Universitaria') ))

```

##1.8 Grafica Revision del Balance de Clases Train
```{r}
# Calcular proporción de clases (No pobre vs Pobre)
tabla_pobreza <- table(train_factors$Pobre)
prop_clases_pobreza <- prop.table(tabla_pobreza)


# Abrir el dispositivo gráfico PNG
png("../04_Views/GRÁFICO1-PROPORCIÓN DE CLASES DE POBREZA EN DATOS DE ENTRENAMIENTO.png", 
    width = 800, height = 600, res = 150)

# Visualizar proporción de clases en un gráfico de barras
bp1 <- barplot(prop_clases_pobreza,
               main = "Proporción de clases de pobreza en datos de entrenamiento",
               xlab = "Condición de pobreza",
               ylab = "Proporción",
               col = c("lightgreen", "coral"),
               names.arg = c("No pobre", "Pobre"),
               ylim = c(0, 1))

# Agregar etiquetas de porcentaje
text(x = bp1, 
     y = prop_clases_pobreza + 0.05,
     labels = paste0(round(prop_clases_pobreza * 100, 1), "%"),
     cex = 0.8)

# Cerrar el dispositivo gráfico
dev.off()
```

##1.9 Rebalanceo de la muestra train
```{r}
# Calcular ponderadores
n_no_pobre <- sum(train_factors$Pobre == "No")
n_pobre <- sum(train_factors$Pobre == "Yes")
pos_weight <- n_no_pobre / n_pobre  # Peso para la clase minoritaria (Pobre)

# Crear vector de pesos
wts <- ifelse(train_factors$Pobre == "Yes", pos_weight, 1)


```

##1.10 Grafica Revision del rebalance de Clases Train
```{r}
# Calcular distribución ponderada
tabla_ponderada <- c(
  sum(wts[train_factors$Pobre == "No"]),  # Suma de pesos para "No pobre"
  sum(wts[train_factors$Pobre == "Yes"])  # Suma de pesos para "Pobre"
)
prop_ponderada <- prop.table(tabla_ponderada)

# Crear dataframe
datos_plot <- data.frame(
  Clase = rep(c("No pobre", "Pobre"), 2),
  Proporcion = c(prop_clases_pobreza, prop_ponderada),
  Tipo = rep(c("Original", "Ponderada"), each = 2)
)

# Crear gráfico de barras agrupadas
ggplot(datos_plot, aes(x = Clase, y = Proporcion, fill = Tipo)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), width = 0.8) +
  geom_text(aes(label = paste0(round(Proporcion * 100, 1), "%")), 
            position = position_dodge(width = 0.9),
            vjust = -0.5, size = 3.5) +
  labs(title = "Proporción de clases: Original vs. Ponderada",
       x = "Condición de pobreza",
       y = "Proporción") +
  scale_fill_manual(values = c("Original" = "lightgreen", "Ponderada" = "coral")) +
  theme_bw() +
  ylim(0, 1)

# Guardar el gráfico con una ruta relativa a la carpeta
ggsave("../04_Views/GRAFICO2-PROPORCION_DE_CLASES.png", dpi = 300)
```

#2. Entrenamiento Modelo Elastic Net
##2.1 Modelo Elastic Net con Caret General
```{r}
#Configacion de la validacion Cruzada
#No tiene la muestra rebalanceada
ctrl_EN_gral <- trainControl(method = "cv",
                    number = 5,
                    classProbs = TRUE,
                    savePredictions = TRUE, 
                    summaryFunction = twoClassSummary
                    )
```

##2.2 Definir la cuadricula de hiperparametros General
```{r}
#Definir la grilla de los hiperparametros alfa y lambda
# El hipeparametro alfa controla Ridge (0) y Lasso (1)
# El hipeparametro lambda controla la intensidad de la regularización

grilla_EN_gral <- expand.grid(
  alpha= c(0.1, 0.3, 0.5, 0.7, 0.9),
  lambda= 10^seq(-4, -1, length = 10) #Logaritmos
)

```

##2.3 Entrenar el modelo Elastic Net con Caret General
```{r}
#Modelo Elastic Net
#Tiempo para ejecucion 6 min

#Entrenamiento del modelo
set.seed(123)

mod_1_EN_gral <- train(Pobre~., #Modelo 1 Elastic Net
    data = train_factors, #Datos de entrenamiento en factores
    method = "glmnet", #Metodo de cálculo Elastic Net
    trControl = ctrl_EN_gral, #Validacion Cruzada
    tuneGrid = grilla_EN_gral, #Grilla de Hiperparametros
    metric = "ROC",
    preProcess = c("center","scale","zv") #Preprocesamiento
    )
```

##2.4 Resultado modelo Elastic Net General
```{r}
# Resumen del modelo entrenado  Elastic Net
print(mod_1_EN_gral)

# Resumen hiperparámetros 
print(mod_1_EN_gral$bestTune)

# Resultados de validación cruzada
plot(mod_1_EN_gral)


```

##2.5 Predicción modelo Elastic Net General
```{r}
#obtener la probabilidad de los datos de entrenamiento
prob_train_gral <- predict(mod_1_EN_gral, 
                      newdata = train_factors, 
                      type = "prob")[,"Yes"]
```

##2.6 Optimización de umbral para maximizar F1 Score General
```{r}
# Calculamos la curva PR para 100 valores de umbral
prec_recall <- data.frame(coords(roc(response = train_factors$Pobre, predictor = prob_train_gral), 
                                seq(0, 1, length=100), 
                                ret=c("threshold", "precision", "recall")))

# Añadir columna de F1 Score
prec_recall$f1_score <- 2 * (prec_recall$precision * prec_recall$recall) / 
                          (prec_recall$precision + prec_recall$recall)

# Encontrar el umbral que maximiza F1
mejor_f1 <- max(prec_recall$f1_score, na.rm = TRUE)
umbral_optimo <- prec_recall$threshold[which.max(prec_recall$f1_score)]

print(paste("Mejor F1 Score:", round(mejor_f1, 4), 
            "con umbral:", round(umbral_optimo, 4)))

# Visualización de Precisión, Recall y F1 para diferentes umbrales
graf_metricas_EN_Gen <- ggplot(prec_recall, aes(x = threshold)) +
  geom_line(aes(y = precision, color = "Precisión")) +
  geom_line(aes(y = recall, color = "Recall")) +
  geom_line(aes(y = f1_score, color = "F1 Score")) +
  geom_vline(xintercept = umbral_optimo, linetype = "dashed") +
  labs(title = "Métricas vs Umbral de Decisión",
       x = "Umbral",
       y = "Valor",
       color = "Métrica") +
  theme_bw()

# Guardar gráfico
ggsave("../04_Views/GRAFICO3-MÉTRICAS_vs_UMBRAL_DECISION.png", plot = graf_metricas_EN_Gen, dpi = 300, width = 8, height = 6)

# Usar el umbral óptimo para las predicciones
umbral_gral <- umbral_optimo  # Usar el umbral optimizado

# Obtener predicciones en los datos de entrenamiento con el umbral optimizado
predicciones_train_gral <- ifelse(prob_train_gral > umbral_gral, "Yes", "No")
predicciones_train_gral <- factor(predicciones_train_gral, levels = levels(train_factors$Pobre))

```

##2.7 Crear la curva ROC para prediccion train set Elastic Net General
```{r}
# Crear objeto ROC

roc_obj_gral <- roc(response = train_factors$Pobre, 
               predictor = prob_train_gral)
# Calcular AUC
auc_value_gral <- auc(roc_obj_gral)

# Calcular punto específico en la curva ROC para el umbral 0.3
conf_matrix <- confusionMatrix(predicciones_train_gral, train_factors$Pobre)
sensibilidad_umbral_gral <- conf_matrix$byClass["Sensitivity"]
especificidad_umbral_gral <- conf_matrix$byClass["Specificity"]

# Extraer datos para ggplot
roc_data_gral <- data.frame(
  Especificidad = 1 - roc_obj_gral$specificities,
  Sensibilidad = roc_obj_gral$sensitivities
)

# Punto para el umbral elegido
punto_umbral_gral <- data.frame(
  Especificidad = 1 - especificidad_umbral_gral,
  Sensibilidad = sensibilidad_umbral_gral
)

# Crear gráfico con ggplot2
graf_roc_EN_gen <- ggplot() +
  geom_line(data = roc_data_gral, aes(x = Especificidad, y = Sensibilidad), 
            color = "blue", size = 1) +
  geom_point(data = punto_umbral_gral, aes(x = Especificidad, y = Sensibilidad), 
             color = "red", size = 3) +
  geom_text(data = punto_umbral_gral, 
            aes(x = Especificidad, y = Sensibilidad, 
                label = paste("Umbral =", umbral_gral)),
            hjust = -0.2, vjust = -0.5, color = "red") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  labs(
    title = "Curva ROC - Modelo Elastic Net",
    subtitle = paste("AUC =", round(auc_value_gral, 4)),
    x = "1 - Especificidad (Tasa de Falsos Positivos)",
    y = "Sensibilidad (Tasa de Verdaderos Positivos)"
  ) +
  theme_minimal() +
  coord_equal() +
  annotate("text", x = 0.75, y = 0.25, 
           label = paste("AUC =", round(auc_value_gral, 4)), 
           color = "blue", size = 5)

# Guardar
ggsave("../04_Views/GRAFICO4-CURVA_ROC_ELASTIC_NET.png", 
       plot = graf_roc_EN_gen, dpi = 300, width = 8, height = 6)

```

##2.8 Matriz de confusion General
```{r}
# Matriz de confusión del modelo Elastic Net
B_EN_gral <- confusionMatrix(predicciones_train_gral, train_factors$Pobre)
B_EN_gral <- B_EN_gral$table

# Explicación de valores matriz de confusión
TN_train_EN_gral <- B_EN_gral[1,1]  # Verdaderos Negativos
FP_train_EN_gral <- B_EN_gral[1,2]  # Falsos Positivos
FN_train_EN_gral <- B_EN_gral[2,1]  # Falsos Negativos
TP_train_EN_gral <- B_EN_gral[2,2]  # Verdaderos Positivos

# Crear tabla de conteos
counts_table_EN_gral <- data.frame(
  categoría = c("Verdaderos Negativos (TN)", "Falsos Positivos (FP)", 
                "Falsos Negativos (FN)", "Verdaderos Positivos (TP)"),
  conteo = c(TN_train_EN_gral, FP_train_EN_gral, FN_train_EN_gral, TP_train_EN_gral)
)

print(counts_table_EN_gral)
```

##2.9 Metricas Modelo Elastic Net General
```{r}
# Calcular métricas
accuracy_train_EN_gral <- (TN_train_EN_gral + TP_train_EN_gral) / (TN_train_EN_gral + FP_train_EN_gral + FN_train_EN_gral + TP_train_EN_gral)
precision_train_EN_gral <- TP_train_EN_gral / (TP_train_EN_gral + FP_train_EN_gral)
recall_train_EN_gral <- TP_train_EN_gral / (TP_train_EN_gral + FN_train_EN_gral)
specificity_train_EN_gral <- TN_train_EN_gral / (TN_train_EN_gral + FP_train_EN_gral)
f1_score_train_EN_gral <- 2 * (precision_train_EN_gral * recall_train_EN_gral) / (precision_train_EN_gral + recall_train_EN_gral)
error_rate_train_EN_gral <- 1 - accuracy_train_EN_gral

# Crear un data frame con los resultados
metrics_table_mod_1_EN_gral <- data.frame(
  metrica_train_cv_gral = c("Accuracy (Exactitud)", "Precision (Precisión)", "Recall (Sensibilidad)", 
              "Specificity (Especificidad)", "F1 Score", "Error Rate (Tasa de error)"),
  Valor_gral = c(accuracy_train_EN_gral, precision_train_EN_gral, recall_train_EN_gral, specificity_train_EN_gral, f1_score_train_EN_gral, error_rate_train_EN_gral),
    Descripción = c(
    "Proporción total de predicciones correctas",
    "De los predichos como pobres, cuántos realmente lo son",
    "De los realmente pobres, cuántos fueron identificados correctamente",
    "De los realmente no pobres, cuántos fueron identificados correctamente",
    "Media armónica entre precisión y recall",
    "Proporción de predicciones incorrectas"
))

# Imprimir tabla de métricas
print(metrics_table_mod_1_EN_gral)
```


##2.10 Estimacion de coeficientes por Elastic Net General
```{r}
# Usar varImp para obtener la importancia de las variables
importancia <- varImp(mod_1_EN_gral)

# Convertir a dataframe y ordenar
imp_df <- data.frame(
  Variable = rownames(importancia$importance),
  Importancia = importancia$importance[,1]
)
imp_df <- imp_df[order(imp_df$Importancia, decreasing = TRUE),]

# Mostrar las 20 variables más importantes
print(head(imp_df, 20))

# Graficar las 15 variables más importantes
top15 <- head(imp_df, 15)

# Crear gráfico con ggplot
grafico_imp_en <- ggplot(top15, aes(x = reorder(Variable, Importancia), y = Importancia)) +
  geom_bar(stat = "identity", fill = "grey") +
  coord_flip() +
  labs(title = "Importancia de Variables - Modelo Elastic Net",
       x = "Variables", y = "Importancia escalada") +
  theme_bw()

# Guardar como imagen PNG
ggsave("../04_Views/GRÁFICO5-IMPORTANCIA-VAR-ELASTIC-NET.png", 
       plot = grafico_imp_en, width = 8, height = 6)


#plot(importancia, top = 15)

```

##2.11 Seleccionar variables importantes y ponderacion
```{r}
#Seleccion de las variables importantes según paso anterior
variables_importantes <- c("ncotizando", "nt_trab_semanal", "Nper", 
                         "nocupados", "nmayor_edad", "pagada", 
                         "H_cotizando", "H_Head_ocupado", "usufructo", 
                         "maxEducLevel", "H_Head_Educ_level", 
                         "Dominio", "sin_titulo", "Pobre")
train_importantes <- train_factors[, names(train_factors) %in% variables_importantes]


```

##2.12 Graficos proporcicion de la muestra variables importantes
```{r}
# Calcular proporción de clases (No pobre vs Pobre)
tabla_pobreza_import <- table(train_importantes$Pobre)
prop_clases_pobreza_import <- prop.table(tabla_pobreza_import)

# Abrir el dispositivo gráfico
png("../04_Views/GRÁFICO5-PROPORCIÓN DE CLASES DE POBRAZA IMPORTANTES EN ENTRENAMIENTO.png", width = 800, height = 600, res = 150)

# Visualizar proporción de clases en un gráfico de barras
barplot(prop_clases_pobreza_import,
        main = "Proporción de clases de pobreza en datos de entrenamiento",
        xlab = "Condición de pobreza",
        ylab = "Proporción",
        col = c("lightgreen", "coral"),
        names.arg = c("No pobre", "Pobre"),
        ylim = c(0, 1))

# Agregar etiquetas de porcentaje
text(x = 1:length(prop_clases_pobreza_import), 
     y = prop_clases_pobreza_import + 0.05,
     labels = paste0(round(prop_clases_pobreza_import * 100, 1), "%"),
     cex = 0.8)

# Cerrar el dispositivo gráfico
dev.off()

# Calcular ponderadores
n_no_pobre_import <- sum(train_importantes$Pobre == "No")
n_pobre_import <- sum(train_importantes$Pobre == "Yes")
pos_weight_import <- n_no_pobre_import / n_pobre_import  # Peso para la clase minoritaria (Pobre)

# Crear vector de pesos
wts_import <- ifelse(train_importantes$Pobre == "Yes", pos_weight_import, 1)

# Calcular distribución ponderada
tabla_ponderada_import <- c(
  sum(wts_import[train_importantes$Pobre == "No"]),  # Suma de pesos para "No pobre"
  sum(wts_import[train_importantes$Pobre == "Yes"])  # Suma de pesos para "Pobre"
)
prop_ponderada_import <- prop.table(tabla_ponderada_import)

# Crear dataframe
datos_plot_import <- data.frame(
  Clase = rep(c("No pobre", "Pobre"), 2),
  Proporcion = c(prop_clases_pobreza_import, prop_ponderada_import),
  Tipo = rep(c("Original", "Ponderada"), each = 2)
)

# Crear gráfico de barras agrupadas
graf_comp_clases <- ggplot(datos_plot_import, aes(x = Clase, y = Proporcion, fill = Tipo)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), width = 0.8) +
  geom_text(aes(label = paste0(round(Proporcion * 100, 1), "%")), 
            position = position_dodge(width = 0.9),
            vjust = -0.5, size = 3.5) +
  labs(title = "Proporción de clases: Original vs. Ponderada",
       x = "Condición de pobreza",
       y = "Proporción") +
  scale_fill_manual(values = c("Original" = "lightgreen", "Ponderada" = "coral")) +
  theme_minimal() +
  ylim(0, 1)

# Guardar 
ggsave("../04_Views/GRAFICO-PROPORCION-CLASES-ORIGINAL-VS-PONDERADA.png", 
       plot = graf_comp_clases, width = 8, height = 6)

```


##2.13 Crear una funcion para el optimizar el F1 Scores
```{r}
f1_summary <- function(data, lev = NULL, model = NULL) {
  # Asegurarse que los niveles estén correctamente identificados
  if(is.null(lev)) lev <- levels(data$obs)
  
  # Identificar la clase positiva (asumimos que es la segunda, "Yes")
  positive_class <- lev[2]
  
  # Extraer las probabilidades de la clase positiva
  probs <- data[, positive_class]
  
  # Convertir a predicciones binarias usando umbral
  pred_class <- factor(ifelse(probs >= 0.3, positive_class, lev[1]), levels = lev)
  
  # Calcular precisión (precision)
  precision <- sum(pred_class == positive_class & data$obs == positive_class) / 
               sum(pred_class == positive_class)
  
  # Calcular exhaustividad (recall)
  recall <- sum(pred_class == positive_class & data$obs == positive_class) / 
            sum(data$obs == positive_class)
  
  # Calcular F1
  f1 <- 2 * (precision * recall) / (precision + recall)
  
  # En caso de NaN (por ejemplo, si precision y recall son 0)
  if(is.nan(f1)) f1 <- 0
  
  # Devolver un vector nombrado con F1
  result <- c(F1 = f1)
  names(result) <- "F1"
  
  return(result)
}
```


##2.13 Entrenar el modelo Elastic Net con Caret variables importantes
```{r}
#Tiempo de ejecucion 15 minutos
#Configacion de la validacion Cruzada
ctrl_EN_import <- trainControl(method = "cv",
                    number = 5,
                    classProbs = TRUE,
                    savePredictions = TRUE, 
                    summaryFunction = f1_summary
                    )

#Definir la grilla de los hiperparametros alfa y lambda
# El hipeparametro alfa controla Ridge (0) y Lasso (1)
# El hipeparametro lambda controla la intensidad de la regularización

grilla_EN_import <- expand.grid(
  alpha= seq(0, 1, by = 0.2),
  lambda= 10^seq(-4, -1, length = 10)) #Logaritmos

# Entrenar un nuevo modelo con el dataset reducido
set.seed(123)
mod_1_EN<- train(
  Pobre ~ .,
  data = train_importantes,
  method = "glmnet",
  weights = wts_import, #Muestra rebalanceada
  trControl = ctrl_EN_import,
  tuneGrid = grilla_EN_import,
  metric = "F1",
  preProcess = c("center", "scale","zv"))

```

##2.14 Resultado modelo Elastic Net
```{r}
# Resumen del modelo entrenado  Elastic Net
print(mod_1_EN)

# Resumen hiperparámetros 
print(mod_1_EN$bestTune)

# Resultados de validación cruzada
plot(mod_1_EN)
```


##2.14 Predicción modelo Elastic Net
```{r}
#obtener la probabilidad de los datos de entrenamiento
prob_train <- predict(mod_1_EN, 
                      newdata = train_importantes, 
                      type = "prob")[,"Yes"]
```


##2.15 Optimización de umbral para maximizar F1 Score
```{r}
# Calculamos la curva PR para 100 valores de umbral
prec_recall <- data.frame(coords(roc(response = train_importantes$Pobre, predictor = prob_train), 
                                seq(0, 1, length=100), 
                                ret=c("threshold", "precision", "recall")))

# Añadir columna de F1 Score
prec_recall$f1_score <- 2 * (prec_recall$precision * prec_recall$recall) / 
                          (prec_recall$precision + prec_recall$recall)

# Encontrar el umbral que maximiza F1
mejor_f1 <- max(prec_recall$f1_score, na.rm = TRUE)
umbral_optimo <- prec_recall$threshold[which.max(prec_recall$f1_score)]

print(paste("Mejor F1 Score:", round(mejor_f1, 4), 
            "con umbral:", round(umbral_optimo, 4)))

# Visualización de Precisión, Recall y F1 para diferentes umbrales
ggplot(prec_recall, aes(x = threshold)) +
  geom_line(aes(y = precision, color = "Precisión")) +
  geom_line(aes(y = recall, color = "Recall")) +
  geom_line(aes(y = f1_score, color = "F1 Score")) +
  geom_vline(xintercept = umbral_optimo, linetype = "dashed") +
  labs(title = "Métricas vs Umbral de Decisión",
       x = "Umbral",
       y = "Valor",
       color = "Métrica") +
  theme_minimal()

# Usar el umbral óptimo para las predicciones
umbral <- umbral_optimo  # Usar el umbral optimizado

# Obtener predicciones en los datos de entrenamiento con el umbral optimizado
predicciones_train <- ifelse(prob_train > umbral, "Yes", "No")
predicciones_train <- factor(predicciones_train, levels = levels(train_importantes$Pobre))

```

##2.16 Crear la curva ROC para prediccion train set Elastic Net
```{r}
# Crear objeto ROC

roc_obj <- roc(response = train_importantes$Pobre, 
               predictor = prob_train)
# Calcular AUC
auc_value <- auc(roc_obj)

# Calcular punto específico en la curva ROC para el umbral 0.3
conf_matrix <- confusionMatrix(predicciones_train, train_importantes$Pobre)
sensibilidad_umbral <- conf_matrix$byClass["Sensitivity"]
especificidad_umbral <- conf_matrix$byClass["Specificity"]

# Extraer datos para ggplot
roc_data <- data.frame(
  Especificidad = 1 - roc_obj$specificities,
  Sensibilidad = roc_obj$sensitivities
)

# Punto para el umbral elegido
punto_umbral <- data.frame(
  Especificidad = 1 - especificidad_umbral,
  Sensibilidad = sensibilidad_umbral
)

# Crear gráfico con ggplot2
ggplot() +
  geom_line(data = roc_data, aes(x = Especificidad, y = Sensibilidad), 
            color = "blue", size = 1) +
  geom_point(data = punto_umbral, aes(x = Especificidad, y = Sensibilidad), 
             color = "red", size = 3) +
  geom_text(data = punto_umbral, 
            aes(x = Especificidad, y = Sensibilidad, 
                label = paste("Umbral =", umbral)),
            hjust = -0.2, vjust = -0.5, color = "red") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  labs(
    title = "Curva ROC - Modelo Elastic Net",
    subtitle = paste("AUC =", round(auc_value, 4)),
    x = "1 - Especificidad (Tasa de Falsos Positivos)",
    y = "Sensibilidad (Tasa de Verdaderos Positivos)"
  ) +
  theme_minimal() +
  coord_equal() +
  annotate("text", x = 0.75, y = 0.25, 
           label = paste("AUC =", round(auc_value, 4)), 
           color = "blue", size = 5)

```

##2.17 Matriz de confusion
```{r}
# Matriz de confusión del modelo Elastic Net
B_EN <- confusionMatrix(predicciones_train, train_importantes$Pobre)
B_EN <- B_EN$table

# Explicación de valores matriz de confusión
TN_train_EN <- B_EN[1,1]  # Verdaderos Negativos
FP_train_EN <- B_EN[1,2]  # Falsos Positivos
FN_train_EN <- B_EN[2,1]  # Falsos Negativos
TP_train_EN <- B_EN[2,2]  # Verdaderos Positivos

# Crear tabla de conteos
counts_table_EN <- data.frame(
  categoría = c("Verdaderos Negativos (TN)", "Falsos Positivos (FP)", 
                "Falsos Negativos (FN)", "Verdaderos Positivos (TP)"),
  conteo = c(TN_train_EN, FP_train_EN, FN_train_EN, TP_train_EN)
)

print(counts_table_EN)
```

##2.18 Metricas Modelo Elastic Net
```{r}
# Calcular métricas
accuracy_train_EN <- (TN_train_EN + TP_train_EN) / (TN_train_EN + FP_train_EN + FN_train_EN + TP_train_EN)
precision_train_EN <- TP_train_EN / (TP_train_EN + FP_train_EN)
recall_train_EN <- TP_train_EN / (TP_train_EN + FN_train_EN)
specificity_train_EN <- TN_train_EN / (TN_train_EN + FP_train_EN)
f1_score_train_EN <- 2 * (precision_train_EN * recall_train_EN) / (precision_train_EN + recall_train_EN)
error_rate_train_EN <- 1 - accuracy_train_EN

# Crear un data frame con los resultados
metrics_table_mod_1_EN <- data.frame(
  metrica_train_cv = c("Accuracy (Exactitud)", "Precision (Precisión)", "Recall (Sensibilidad)", 
              "Specificity (Especificidad)", "F1 Score", "Error Rate (Tasa de error)"),
  Valor = c(accuracy_train_EN, precision_train_EN, recall_train_EN, specificity_train_EN, f1_score_train_EN, error_rate_train_EN),
    Descripción = c(
    "Proporción total de predicciones correctas",
    "De los predichos como pobres, cuántos realmente lo son",
    "De los realmente pobres, cuántos fueron identificados correctamente",
    "De los realmente no pobres, cuántos fueron identificados correctamente",
    "Media armónica entre precisión y recall",
    "Proporción de predicciones incorrectas"
))

# Imprimir tabla de métricas
print(metrics_table_mod_1_EN)
```

## 2.19 Comparacion entre el modelo general y el modelo de variables importantes

```{r}
# 1. Comparación de métricas
comparacion_metricas <- data.frame(
  Métrica = c("Accuracy", "Precision", "Recall", "Specificity", "F1 Score", "AUC"),
  Modelo_General = c(
    accuracy_train_EN_gral, 
    precision_train_EN_gral, 
    recall_train_EN_gral, 
    specificity_train_EN_gral, 
    f1_score_train_EN_gral,
    auc_value_gral
  ),
  Modelo_Variables_Importantes = c(
    accuracy_train_EN, 
    precision_train_EN, 
    recall_train_EN, 
    specificity_train_EN, 
    f1_score_train_EN,
    auc_value
  )
)

# Reordenar datos para visualización
comparacion_metricas$Diferencia <- comparacion_metricas$Modelo_Variables_Importantes - comparacion_metricas$Modelo_General
comparacion_metricas$Mejora_Porcentual <- (comparacion_metricas$Diferencia / comparacion_metricas$Modelo_General) * 100

# Mostrar tabla de comparación formateada
comparacion_formateada <- comparacion_metricas %>%
  mutate(
    Modelo_General = round(Modelo_General, 4),
    Modelo_Variables_Importantes = round(Modelo_Variables_Importantes, 4),
    Diferencia = round(Diferencia, 4),
    Mejora_Porcentual = round(Mejora_Porcentual, 2)
  )

print(comparacion_formateada)

# 2. Comparación gráfica de métricas
comparacion_long <- comparacion_metricas %>%
  tidyr::pivot_longer(
    cols = c(Modelo_General, Modelo_Variables_Importantes),
    names_to = "Modelo",
    values_to = "Valor"
  ) %>%
  filter(Métrica != "AUC")  # AUC se visualizará en la curva ROC

# Gráfico comparativo de métricas
ggplot(comparacion_long, aes(x = Métrica, y = Valor, fill = Modelo)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), width = 0.8) +
  geom_text(aes(label = round(Valor, 3)), 
            position = position_dodge(width = 0.9),
            vjust = -0.5, size = 3) +
  labs(title = "Comparación de métricas entre modelos",
       subtitle = "Modelo general vs. Modelo con variables importantes",
       x = "Métrica",
       y = "Valor") +
  scale_fill_manual(values = c("Modelo_General" = "skyblue", 
                               "Modelo_Variables_Importantes" = "coral")) +
  theme_minimal() +
  ylim(0, max(comparacion_long$Valor) * 1.1)

# 3. Comparación de curvas ROC
# Extraer datos para ambas curvas ROC
roc_data_general <- data.frame(
  Especificidad = 1 - roc_obj_gral$specificities,
  Sensibilidad = roc_obj_gral$sensitivities,
  Modelo = "Modelo General"
)

roc_data_vars_imp <- data.frame(
  Especificidad = 1 - roc_obj$specificities,
  Sensibilidad = roc_obj$sensitivities,
  Modelo = "Modelo Variables Importantes"
)

# Combinar datos de ambas curvas
roc_data_combined <- rbind(roc_data_general, roc_data_vars_imp)

# Puntos para los umbrales elegidos
puntos_umbral <- data.frame(
  Especificidad = c(1 - especificidad_umbral_gral, 1 - especificidad_umbral),
  Sensibilidad = c(sensibilidad_umbral_gral, sensibilidad_umbral),
  Modelo = c("Modelo General", "Modelo Variables Importantes"),
  Umbral = c(umbral_gral, umbral)
)

# Crear gráfico comparativo de curvas ROC
ggplot() +
  geom_line(data = roc_data_combined, 
            aes(x = Especificidad, y = Sensibilidad, color = Modelo), 
            size = 1) +
  geom_point(data = puntos_umbral, 
             aes(x = Especificidad, y = Sensibilidad, color = Modelo),
             size = 3) +
  geom_text(data = puntos_umbral,
            aes(x = Especificidad, y = Sensibilidad, 
                label = paste("Umbral =", round(Umbral, 3)), color = Modelo),
            hjust = -0.1, vjust = -0.5, size = 3) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  labs(
    title = "Comparación de Curvas ROC",
    subtitle = paste("AUC General =", round(auc_value_gral, 4), 
                     "| AUC Variables Importantes =", round(auc_value, 4)),
    x = "1 - Especificidad (Tasa de Falsos Positivos)",
    y = "Sensibilidad (Tasa de Verdaderos Positivos)"
  ) +
  scale_color_manual(values = c("Modelo General" = "skyblue", 
                               "Modelo Variables Importantes" = "coral")) +
  theme_minimal() +
  coord_equal() +
  annotate("text", x = 0.75, y = 0.25, 
           label = paste("Mejora en AUC:", 
                         round((auc_value - auc_value_gral) * 100, 2), "%"), 
           color = "darkgreen", size = 4)

# 4. Comparación de hiperparámetros óptimos
hiperparametros <- data.frame(
  Modelo = c("Modelo General", "Modelo Variables Importantes"),
  Alpha = c(mod_1_EN_gral$bestTune$alpha, mod_1_EN$bestTune$alpha),
  Lambda = c(mod_1_EN_gral$bestTune$lambda, mod_1_EN$bestTune$lambda)
)

print(hiperparametros)

# 5. Comparación de matrices de confusión
# Convertir matrices de confusión a formato largo para visualización
conf_mat_general <- as.data.frame.matrix(B_EN_gral)
conf_mat_general$Predicted <- rownames(conf_mat_general)
conf_mat_general_long <- reshape2::melt(conf_mat_general, id.vars = "Predicted", 
                                       variable.name = "Actual", value.name = "Count")
conf_mat_general_long$Modelo <- "Modelo General"

conf_mat_vars_imp <- as.data.frame.matrix(B_EN)
conf_mat_vars_imp$Predicted <- rownames(conf_mat_vars_imp)
conf_mat_vars_imp_long <- reshape2::melt(conf_mat_vars_imp, id.vars = "Predicted", 
                                        variable.name = "Actual", value.name = "Count")
conf_mat_vars_imp_long$Modelo <- "Modelo Variables Importantes"

# Combinar datos de ambas matrices
conf_mat_combined <- rbind(conf_mat_general_long, conf_mat_vars_imp_long)

# Calcular porcentajes dentro de cada matriz
conf_mat_combined <- conf_mat_combined %>%
  group_by(Modelo) %>%
  mutate(Total = sum(Count),
         Percentage = Count / Total * 100)

# Crear gráficos de heatmap para matrices de confusión
ggplot(conf_mat_combined, aes(x = Actual, y = Predicted, fill = Count)) +
  geom_tile() +
  facet_wrap(~Modelo) +
  geom_text(aes(label = paste0(Count, "\n(", round(Percentage, 1), "%)"))) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Comparación de Matrices de Confusión",
       x = "Clase Real",
       y = "Clase Predicha") +
  theme_minimal()
```


##2.20 Prediccion con el Test Set Elastic Net
```{r}
#Prediccion el Test Set
predictTest_EN <- test_factors %>%
  mutate(prob_probre_EN = predict(mod_1_EN,
         newdata = test_factors,
         type = "prob")[,"Yes"],
         pred_probre_cat_EN = ifelse(prob_probre_EN > umbral, "Yes", "No"), #Cambio del umbral de referencia
         pred_probre_EN = ifelse(pred_probre_cat_EN == "Yes", 1, 0)# Predicción numérica (1/0)
         )

# Resumen verificación
summary(predictTest_EN$prob_probre_EN)
table(predictTest_EN$pred_probre_cat_EN)
table(predictTest_EN$pred_probre_EN)

# Guardar los resultados
resultados_prediccion_EN<- predictTest_EN %>%
  select(id, pred_probre_EN)

# Ver las primeras filas de los resultados
head(resultados_prediccion_EN)

# Crear un resumen de las predicciones del modelo Elastic Net
prediccion_test_EN <- data.frame(
  Estadística = c("Número total de predicciones", 
                 "Hogares clasificados como pobres", 
                 "Hogares clasificados como no pobres",
                 "Porcentaje clasificado como pobre"),
  Valor = c(nrow(resultados_prediccion_EN),
           sum(resultados_prediccion_EN$pred_probre_EN),
           nrow(resultados_prediccion_EN) - sum(resultados_prediccion_EN$pred_probre_EN),
           sprintf("%.2f%%", 100 * sum(resultados_prediccion_EN$pred_probre_EN) / nrow(resultados_prediccion_EN)))
)
print(prediccion_test_EN)
```

##2.21 Gráficos de la predicción
```{r}
# Distribucion de probabilidades predichas
hist(predictTest_EN$prob_probre_EN, 
     main = "Distribución de probabilidades", 
     xlab = "Probabilidad de ser clasificado como pobre",
     col = "lightblue",
     border = "white",
     breaks = 20)  # Puedes ajustar el número de rangos

# Proporción de clases predichas
prop_clases_EN <- prop.table(table(predictTest_EN$pred_probre_EN))
print(prop_clases_EN)

# Visualizar proporción de clases en un gráfico de barras
barplot(prop_clases_EN,
        main = "Proporción de clases predichas",
        xlab = "Clase (0 = No pobre, 1 = Pobre)",
        ylab = "Proporción",
        col = c("lightgreen", "coral"),
        names.arg = c("No pobre", "Pobre"),
        ylim = c(0, 1))

# Agregar etiquetas de porcentaje
text(x = 1:length(prop_clases_EN), 
     y = prop_clases_EN + 0.05,
     labels = paste0(round(prop_clases_EN * 100, 1), "%"),
     cex = 0.8)


```

##2.22 Template Elastic Net
```{r}
write.csv(resultados_prediccion_EN, "../05_Predictions/09042025_sample_equipo4_EN.csv", 
          row.names = FALSE,
          quote = FALSE)
```


#3. Entrenamiento Modelo Logit

##3.1 Estimadores logit
```{r}
#Calculo de los estimadores Logit
mod_1_logit<- glm(Pobre~., 
                 data = train_importantes, 
                 family = "binomial",
                 weights = wts_import) # Inclusion de penderacion de clases

```

##3.2 Resultados Estimadores Logit 

```{r}
summary(mod_1_logit,type="text")
```

##3.3 Probabilidad Logit

```{r}
#Calculo de probabilidades
prob_logit<- train_importantes %>% 
  mutate(prob_hat=predict(mod_1_logit,
                          newdata = train_importantes, 
                          type = "response"))

```

## 3.4 Resultado Probabilidad Logit

```{r}
head(prob_logit %>% 
       select(Pobre,prob_hat))
```

## 3.5 Clasificador Logit

```{r}

umbral # Bayes Rule #Ver variable Umbral

prob_logit<- prob_logit %>% 
  mutate(pred_pobre=ifelse(prob_hat>umbral,1,0))    ## prediccion de pobre (0) No pobre y (1) Pobre

head(prob_logit %>%
       select(Pobre,prob_hat,pred_pobre))

```

##3.6.1 Matriz de confusion

```{r}
# Convertir la variable Pred_pobre a factor con etiquetas descriptivas
prob_logit$pred_pobre_factor <- factor(prob_logit$pred_pobre,
                                      levels = c(0, 1),
                                      labels = c("No Pobre", "Pobre"))

#Creacion de una matriz de confuncion
#           Predicho
# Real       0    1
#       0   [TN] [FP]
#       1   [FN] [TP]

#Matriz de confusión
A <- with(prob_logit,table(Pobre,pred_pobre_factor))

# Mostrar matriz de confusión
print("Matriz de confusión:")
print(A)

# Explicacion de valores matriz de confusion
TN_train <- A[1]  # Verdaderos Negativos
FP_train <- A[2]  # Falsos Positivos
FN_train <- A[3]  # Falsos Negativos
TP_train <- A[4]  # Verdaderos Positivos

# Crear tabla de conteos
counts_table_train <- data.frame(
  Categoría = c("Verdaderos Negativos (TN)", "Falsos Positivos (FP)", 
                "Falsos Negativos (FN)", "Verdaderos Positivos (TP)"),
  Conteo = c(TN_train, FP_train, FN_train, TP_train)
)

print(counts_table_train)


```

##3.6.2 Metricas Modelo Logit

```{r}
#Métricas derivadas:
# Exactitud (Accuracy): (TP + TN) / (TP + TN + FP + FN)
# Proporción de predicciones correctas.
# Precisión (Precision): TP / (TP + FP)
# De todos los casos predichos como "Pobres", ¿cuántos realmente lo son?
# Sensibilidad/Recall: TP / (TP + FN)
# De todos los casos realmente "Pobres", ¿cuántos identificamos correctamente?
# Especificidad: TN / (TN + FP)
# De todos los casos realmente "No pobres", ¿cuántos identificamos correctamente?
# F1-Score: 2 * (Precisión * Recall) / (Precisión + Recall)
# Media armónica de precisión y recall.

# Accuracy
accuracy_train <- (TN_train+ TP_train) / (TN_train + FP_train + FN_train + TP_train)
# Precisión
precision_train <- TP_train / (TP_train + FP_train)
# Recall (Sensibilidad)
recall_train <- TP_train / (TP_train + FN_train)
# Especificidad
specificity_train <- TN_train / (TN_train + FP_train)
# F1 Score
f1_score_train <- 2 * (precision_train * recall_train) / (precision_train + recall_train)
# Error Rate (Tasa de error)
error_rate_train <- 1 - accuracy_train


# Crear tabla de métricas
metrics_table_mod_1_log <- data.frame(
  metrica_train = c("Accuracy (Exactitud)", "Precision (Precisión)", "Recall (Sensibilidad)", 
              "Specificity (Especificidad)", "F1 Score", "Error Rate (Tasa de error)"),
  Valor = c(accuracy_train, precision_train, recall_train, specificity_train, f1_score_train, error_rate_train),
  Descripción = c(
    "Proporción total de predicciones correctas",
    "De los predichos como pobres, cuántos realmente lo son",
    "De los realmente pobres, cuántos fueron identificados correctamente",
    "De los realmente no pobres, cuántos fueron identificados correctamente",
    "Media armónica entre precisión y recall",
    "Proporción de predicciones incorrectas"
  )
)

# Mostrar tabla
print(metrics_table_mod_1_log)

```

##3.7 Entrenamiento Fuera de muestra Modelo_Logit

```{r}
ctrl_logit<- trainControl(method = "cv",
                    number = 5,
                    classProbs = TRUE,
                    savePredictions = TRUE,
                    verbose=TRUE
                    )
```

##3.8 Entrenamiento modelo 2

```{r}
#Entrenamiento del modelos Logit 
#Tiempo de ejecucion 35 seg
set.seed(123)
mod_2_logit <- train(Pobre~. ,
                     data = train_importantes, 
                     method = "glm",
                     family = "binomial",
                     trControl = ctrl_logit,
                     metric = "Accuracy",
                     weights = wts_import # Inclusion de penderacion de clases
                     )

# Ver resultados
print(mod_2_logit)
summary(mod_2_logit)

# Ver predicciones por cada fold
head(mod_2_logit$pred)

```

##3.9 Matriz de confusion modelo 2

```{r}
# Obtener predicciones en los datos de entrenamiento
predicciones_train_cv <- predict(mod_2_logit, 
                                 newdata = train_importantes,
                                 type = "prob")

umbral # Bayes Rule #Ver variable Umbral

predicciones_umbral <- factor(ifelse(predicciones_train_cv[,"Yes"] > umbral, 
                                    "Yes", "No"), 
                             levels = levels(train_factors$Pobre))

```

##3.10 Crear la curva ROC para prediccion train set Logit

```{r}
# Crear objeto ROC
roc_obj_logit <- roc(response = train_factors$Pobre, 
               predictor = prob_train)

# Calcular AUC
auc_value_logit <- auc(roc_obj)

# Calcular punto específico en la curva ROC para el umbral elegido
conf_matrix_logit <- confusionMatrix(predicciones_train, train_factors$Pobre)
sensibilidad_umbral_logit <- conf_matrix$byClass["Sensitivity"]
especificidad_umbral_logit <- conf_matrix$byClass["Specificity"]

# Extraer datos para ggplot
roc_data_logit <- data.frame(
  Especificidad = 1 - roc_obj_logit$specificities,
  Sensibilidad = roc_obj_logit$sensitivities
)

# Punto para el umbral elegido
punto_umbral_logit <- data.frame(
  Especificidad = 1 - especificidad_umbral,
  Sensibilidad = sensibilidad_umbral
)

# Crear gráfico con ggplot2
ggplot() +
  geom_line(data = roc_data_logit, aes(x = Especificidad, y = Sensibilidad), 
            color = "blue", size = 1) +
  geom_point(data = punto_umbral_logit, aes(x = Especificidad, y = Sensibilidad), 
             color = "red", size = 3) +
  geom_text(data = punto_umbral_logit, 
            aes(x = Especificidad, y = Sensibilidad, 
                label = paste("Umbral =", umbral)),
            hjust = -0.2, vjust = -0.5, color = "red") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  labs(
    title = "Curva ROC - Modelo Logístico con Validación Cruzada",
    subtitle = paste("AUC =", round(auc_value, 4)),
    x = "1 - Especificidad (Tasa de Falsos Positivos)",
    y = "Sensibilidad (Tasa de Verdaderos Positivos)"
  ) +
  theme_minimal() +
  coord_equal() +
  annotate("text", x = 0.75, y = 0.25, 
           label = paste("AUC =", round(auc_value, 4)), 
           color = "blue", size = 5)
```

##3.11 Matriz de Confusion Logit

```{r}
# Matriz de confusión del modelo con validación cruzada
B_cv <- confusionMatrix(predicciones_umbral, train_importantes$Pobre)
B_cv <- B_cv$table


# Explicacion de valores matriz de confusion
TN_train_cv <- B_cv[1,1]  # Verdaderos Negativos
FP_train_cv <- B_cv[1,2]  # Falsos Positivos
FN_train_cv <- B_cv[2,1]  # Falsos Negativos
TP_train_cv <- B_cv[2,2]  # Verdaderos Positivos

# Crear tabla de conteos
counts_table_cv <- data.frame(
  categoría = c("Verdaderos Negativos (TN)", "Falsos Positivos (FP)", 
                "Falsos Negativos (FN)", "Verdaderos Positivos (TP)"),
  conteo = c(TN_train_cv, FP_train_cv, FN_train_cv, TP_train_cv)
)
print(counts_table_cv)

# Calcular el total de observaciones
total_obs_cv <- sum(counts_table_cv$conteo)

# Añadir porcentajes a la tabla
counts_table_cv$porcentaje <- sprintf("%.2f%%", 100 * counts_table_cv$conteo / total_obs_cv)
print(counts_table_cv)

```

##3.12 Metricas Modelo de Entrenamieno Logit 

```{r}
#Métricas derivadas:
# Exactitud (Accuracy): (TP + TN) / (TP + TN + FP + FN)
# Proporción de predicciones correctas.
# Precisión (Precision): TP / (TP + FP)
# De todos los casos predichos como "Pobres", ¿cuántos realmente lo son?
# Sensibilidad/Recall: TP / (TP + FN)
# De todos los casos realmente "Pobres", ¿cuántos identificamos correctamente?
# Especificidad: TN / (TN + FP)
# De todos los casos realmente "No pobres", ¿cuántos identificamos correctamente?
# F1-Score: 2 * (Precisión * Recall) / (Precisión + Recall)
# Media armónica de precisión y recall.

# Calcular métricas

# Accuracy
accuracy_train_cv <- (TN_train_cv + TP_train_cv) / (TN_train_cv + FP_train_cv + FN_train_cv + TP_train_cv)
# Precisión
precision_train_cv <- TP_train_cv / (TP_train_cv + FP_train_cv)
# Recall (Sensibilidad)
recall_train_cv <- TP_train_cv / (TP_train_cv + FN_train_cv)
# Especificidad
specificity_train_cv <- TN_train_cv / (TN_train_cv + FP_train_cv)
# F1 Score
f1_score_train_cv <- 2 * (precision_train_cv * recall_train_cv) / (precision_train_cv + recall_train_cv)
# Error Rate (Tasa de error)
error_rate_train_cv <- 1 - accuracy_train_cv

# # Crear tabla de métricas
metrics_table_mod_2_log <- data.frame(
  metrica_train_cv = c("Accuracy (Exactitud)", "Precision (Precisión)", "Recall (Sensibilidad)", 
              "Specificity (Especificidad)", "F1 Score", "Error Rate (Tasa de error)"),
  Valor = c(accuracy_train_cv, precision_train_cv, recall_train_cv, specificity_train_cv, f1_score_train_cv, error_rate_train_cv),
    Descripción = c(
    "Proporción total de predicciones correctas",
    "De los predichos como pobres, cuántos realmente lo son",
    "De los realmente pobres, cuántos fueron identificados correctamente",
    "De los realmente no pobres, cuántos fueron identificados correctamente",
    "Media armónica entre precisión y recall",
    "Proporción de predicciones incorrectas"
  )
)

print(metrics_table_mod_2_log)
```

##3.13 Explorar la importancia de las variables

```{r}
# Importancia de variables
importancia <- varImp(mod_2_logit, scale = TRUE)
print(importancia)
plot(importancia, top = 20, main = "Las variables más importantes")
```

##3.14 Comparacion entre los modelos 1 y 2

```{r}
#Crear un dataframe con los dos modelos y verlos en paralelo
comparar_df <- data.frame(
  Métrica = c("Accuracy (Exactitud)", "Precision (Precisión)", "Recall (Sensibilidad)", 
              "Specificity (Especificidad)", "F1 Score", "Error Rate (Tasa de error)"),
  Modelo_Original = c(accuracy_train, precision_train, recall_train, 
                     specificity_train, f1_score_train, error_rate_train),
  Modelo_CV = c(accuracy_train_cv, precision_train_cv, recall_train_cv, 
               specificity_train_cv, f1_score_train_cv, error_rate_train_cv),
  Diferencia = c(accuracy_train_cv - accuracy_train, 
                precision_train_cv - precision_train,
                recall_train_cv - recall_train,
                specificity_train_cv - specificity_train,
                f1_score_train_cv - f1_score_train,
                error_rate_train_cv - error_rate_train)
)

comparar_df$Modelo_Original <- sprintf("%.2f%%", comparar_df$Modelo_Original * 100)
comparar_df$Modelo_CV <- sprintf("%.2f%%", comparar_df$Modelo_CV * 100)
comparar_df$Diferencia <- sprintf("%+.2f%%", comparar_df$Diferencia * 100)

print(comparar_df)

```

##3.15 Predición con el Test Set/Clasificacion 

```{r}
#Prediccion el Test Set
predictTest_logit <- test_factors %>% 
  mutate(
    prob_pobre_logit = predict(mod_2_logit, 
                         newdata = test_factors, 
                         type = "prob")[,"Yes"],# Probabilidad de ser "Pobre" ("Yes")
    pred_pobre_cat_logit = ifelse(prob_pobre_logit > umbral, "Yes", "No"), #Cambio del umbral de referencia
    pred_pobre_logit = ifelse(pred_pobre_cat_logit == "Yes", 1, 0)# Predicción numérica (1/0)
  )

# Resumen
summary(predictTest_logit$prob_pobre_logit)
table(predictTest_logit$pred_pobre_cat_logit)
table(predictTest_logit$pred_pobre_logit)

# Guardar los resultados
resultados_prediccion_logit<- predictTest_logit %>%
  select(id, pred_pobre_logit)

# Ver las primeras filas de los resultados
head(resultados_prediccion_logit)

# Crear un resumen de las predicciones
prediccion_test_logit <- data.frame(
  Estadística = c("Número total de predicciones", 
                 "Hogares clasificados como pobres", 
                 "Hogares clasificados como no pobres",
                 "Porcentaje clasificado como pobre"),
  Valor = c(nrow(resultados_prediccion_logit),
           sum(resultados_prediccion_logit$pred_pobre_logit),
           nrow(resultados_prediccion_logit) - sum(resultados_prediccion_logit$pred_pobre_logit),
           sprintf("%.2f%%", 100 * sum(resultados_prediccion_logit$pred_pobre_logit) / nrow(resultados_prediccion_logit)))
)

print(prediccion_test_logit)

```

##3.16 Gráficos de la predicción

```{r}
# Distribucion de probabilidades predichas
hist(predictTest_logit$prob_pobre_logit, 
     main = "Distribución de probabilidades", 
     xlab = "Probabilidad de ser clasificado como pobre",
     col = "lightblue",
     border = "white",
     breaks = 20)  # Puedes ajustar el número de rangos

# Proporción de clases predichas
prop_clases <- prop.table(table(predictTest_logit$pred_pobre_logit))
print(prop_clases)

# Visualizar proporción de clases en un gráfico de barras
barplot(prop_clases,
        main = "Proporción de clases predichas",
        xlab = "Clase (0 = No pobre, 1 = Pobre)",
        ylab = "Proporción",
        col = c("lightgreen", "coral"),
        names.arg = c("No pobre", "Pobre"),
        ylim = c(0, 1))

# Agregar etiquetas de porcentaje
text(x = 1:length(prop_clases), 
     y = prop_clases + 0.05,
     labels = paste0(round(prop_clases * 100, 1), "%"),
     cex = 0.8)

```

##3.17 Comparacion entre entrenamiento y prediccion

```{r}
comparacion_mod_log <- data.frame(
  Estadística = c(
    "Hogares clasificados como pobres",
    "Hogares clasificados como no pobres",
    "Porcentaje clasificado como pobre"
  ),
  
  Modelo_Original = c(
    TP_train + FP_train,
    TN_train + FN_train,
    sprintf("%.2f%%", (TP_train + FP_train) / (TN_train + FP_train + FN_train + TP_train) * 100)
  ),
  
  Modelo_CV = c(
    FP_train_cv + TP_train_cv,
    TN_train_cv + FN_train_cv,
    sprintf("%.2f%%", 100 * (FP_train_cv + TP_train_cv) / total_obs_cv)
  ),
  
  Predicción_Test = c(
           sum(resultados_prediccion_logit$pred_pobre_logit),
           nrow(resultados_prediccion_logit) - sum(resultados_prediccion_logit$pred_pobre_logit),
           sprintf("%.2f%%", 100 * sum(resultados_prediccion_logit$pred_pobre_logit) / nrow(resultados_prediccion_logit)))
  )

print(comparacion_mod_log)
```

##3.18 Template_Logit

```{r}
write.csv(resultados_prediccion_logit, "../05_Predictions/08042025sample_equipo4_Logit.csv", 
          row.names = FALSE, 
          quote = FALSE)
```


# 4. Entrenamiento Ramdon Forest

## 4.1 Ajustar la base test_factors con la misma estructura que train factors

```{r}
# Verificar si 'Pobre' falta en test_factors y agregarlo correctamente
if (!("Pobre" %in% names(test_factors))) {
  test_factors$Pobre <- factor(NA, levels = levels(train_factors$Pobre))
}

# Asegurar que test_factors tenga todas las variables de train_factors
#issing_vars <- setdiff(names(train_factors), names(test_factors))
#for (var in missing_vars) {
#  test_factors[[var]] <- NA  
#}

# Reordenar columnas para que coincidan
#test_factors <- test_factors[, names(train_factors), drop = FALSE]

# Ajustar niveles de factores
for (col in names(test_factors)) {
  if (is.factor(train_factors[[col]])) {
    test_factors[[col]] <- factor(test_factors[[col]], levels = levels(train_factors[[col]]))
  }
}

# Verificar si Pobre ya no es puro NA
table(test_factors$Pobre, useNA = "always")

```

## 4.2 Entrenamiento modelo con randomForest (sin crossvalidation)

```{r}
set.seed(123)

randomforest <- randomForest::randomForest(Pobre ~ . ,
               data = train_factors, 
               mtry = 4,  # Valor óptimo encontrado en CV
               ntree = 500,  # Número de árboles
               importance = TRUE)  # Para ver la importancia de variables
          
# Ver resultados
print(randomforest)
```

## 4.3 Predicción con el modelo Random Forest

```{r}
# Predicción en test_factors excluyendo 'id'
predictTest_rf <- test_factors %>% 
  mutate(
    prob_pobre_rf = predict(
      randomforest, 
      newdata = test_factors[, setdiff(names(test_factors), "id")], 
      type = "prob"
    )[,"Yes"],

    pred_pobre_cat_rf = ifelse(prob_pobre_rf > 0.5, "Yes", "No"),
    pobre = ifelse(pred_pobre_cat_rf == "Yes", 1, 0)
  )

# Guardar resultados para Kaggle con el nombre Pobre
resultados_prediccion_rf <- predictTest_rf %>%
  select(id, pobre)

```

## 4.4 Template_RF

```{r}
## Template_RF
write.csv(resultados_prediccion_rf, "C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller2/T2_BDML/05_Predictions/080425sample_equipo4_rf.csv", 
          row.names = FALSE,
          quote = FALSE)
```

## 4.5 Ver importancia de variables RF

```{r}

# Ver importancia de variables
randomForest::varImpPlot(randomforest, main = "Importancia de Variables (Random Forest)")

png("C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller2/T2_BDML/04_Views/GRAFICO_IMPOVARI_RF.png",
    width = 900, height = 600)

par(mfrow = c(1, 2), mar = c(5, 8, 4, 2))  
randomForest::varImpPlot(randomforest, main = "Importancia de Variables (Random Forest)")

dev.off()

```

## 4.6 Entrenamiento fuera de muestra cross validation con ranger

```{r}
# Fijar semilla
set.seed(123)

fiveStats <- function(...) {
  c(
    caret::twoClassSummary(...),  # ROC, Sensibilidad, Especificidad
    caret::defaultSummary(...)    # Accuracy y Kappa
  )
}

ctrl_rf_cv <- trainControl(
  method = "cv",             # cross validation
  number = 5,                
  summaryFunction = fiveStats,
  classProbs = TRUE,         #probabilidades
  verbose = FALSE,
  savePredictions = TRUE
)

# Definimos el grid sobre la que se realizará la busqueda de hiperparámetros
mtry_grid <- expand.grid(
  mtry = 4,
  splitrule = "gini",
  min.node.size = 1
)

```

## 4.7 Entrenamiento del modelo para buscar la mejor combinación de parámetros, se usa ranger con cross validation

```{r}
# Crear versión temporal sin Dominio ni id
train_rf <- train_factors %>% 
  select(-Dominio)

# Entrenamiento
cv_rf <- train(
  Pobre ~ .,
  data = train_rf,
  method = "ranger",
  metric = "ROC",
  tuneGrid = mtry_grid,
  trControl = ctrl_rf_cv,
  num.trees = 500,
  importance = "impurity"
)


```

## 4.8 Predicciones RF con libreria ranger + cv

```{r}
# Predicción en test_factors excluyendo 'id'
predictTest_rf_cv <- test_factors %>% 
  mutate(
    prob_pobre_rf_cv = predict(
      cv_rf, 
      newdata = test_factors[, setdiff(names(test_factors), "id")], 
      type = "prob"
    )[,"Yes"],

    pred_pobre_cat_rf_cv = ifelse(prob_pobre_rf_cv > 0.5, "Yes", "No"),
    pobre = ifelse(pred_pobre_cat_rf_cv == "Yes", 1, 0)
  )

# Guardar resultados para Kaggle con el nombre solicitado
resultados_prediccion_rf_cv <- predictTest_rf_cv %>%
  select(id, pobre)

```

## 4.9 Template_RF_CV

```{r}
## Template_RF
write.csv(resultados_prediccion_rf_cv, "C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller2/T2_BDML//05_Predictions/080425sample_equipo4_rf_cv.csv", 
          row.names = FALSE,
          quote = FALSE)
```

## 4.10 Ver importancia de variables

```{r}
# Crear gráfico de importancia de variables
varimp_plot <- plot(varImp(cv_rf), top = 10)

# Guardar el gráfico
png("C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller2/T2_BDML/04_Views/GRÁFICO-VARIABLE IMPORTANTE_rf_cv.png", width = 800, height = 600)
plot(varImp(cv_rf), top = 10)
dev.off()

# Mostrar gráfico
plot(varImp(cv_rf), top = 10)

```

## 4.11 Comparar AUC de ambos modelos

```{r}
# AUC para modelo RandomForest sin CV
aucval_rf <- Metrics::auc(
  actual = predictTest_rf$pobre,
  predicted = predictTest_rf$prob_pobre_rf
)

# AUC para modelo RandomForest con CV (ranger)
aucval_rf_cv <- Metrics::auc(
  actual = predictTest_rf_cv$pobre,
  predicted = predictTest_rf_cv$prob_pobre_rf_cv
)

# Mostrar resultados
print(paste("AUC RandomForest: ", round(aucval_rf, 4)))
print(paste("AUC RandomForest con CV (ranger): ", round(aucval_rf_cv, 4)))



```

# 4.12 Mejorando predicción ajustando por peso
```{r}
set.seed(123)

cv_rf_weighted <- train(
  Pobre ~ .,
  data = train_importantes,
  method = "ranger",
  metric = "ROC",
  trControl = ctrl_rf_cv,     
  tuneGrid = mtry_grid,       
  weights = wts_import,       
  num.trees = 500,
  importance = "impurity"
)

```
## 4.13 Predicciones 
```{r}
# Predicción en test_factors
predictTest_rf_weighted <- test_factors %>% 
  mutate(
    prob_pobre_rf_weighted = predict(
      cv_rf_weighted, 
      newdata = test_factors[, setdiff(names(test_factors), "id")], 
      type = "prob"
    )[,"Yes"],

    pred_pobre_cat_rf_weighted = ifelse(prob_pobre_rf_weighted > 0.5, "Yes", "No"),
    pobre = ifelse(pred_pobre_cat_rf_weighted == "Yes", 1, 0)
  )

# Guardar resultados
resultados_prediccion_rf_weighted <- predictTest_rf_weighted %>%
  select(id, pobre)
```

## 4.14 Template_RF_CV_wighted

```{r}
## Template_RF
write.csv(resultados_prediccion_rf_weighted, "C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller2/T2_BDML/05_Predictions/080425sample_equipo4_rf_cv_wg.csv", row.names = FALSE)
```

## 4.15 Mejorando la predicción 
```{r}
# Variables al cuadrado
test_rf_final <- test_factors %>%
  mutate(Nper_sq = Nper^2)

train_importantes <- train_importantes %>%
  mutate(Nper_sq = Nper^2)

```

## 4.16 Entrenando el modelo mejorado
```{r}
set.seed(123)

cv_rf_weighted_quad <- train(
  Pobre ~ .,
  data = train_importantes,
  method = "ranger",
  metric = "ROC",
  trControl = ctrl_rf_cv,
  tuneGrid = mtry_grid,         
  weights = wts_import,
  num.trees = 500,
  importance = "impurity"
)


```

```{r}
cv_rf_weighted_quad$results  # Te muestra ROC, Sensibilidad, Especificidad, etc.
```
## 4.17 Predicciones
```{r}
## 4.13 Predicciones 
# Predicción en test_factors usando umbral_gral
predictTest_rf_weighted_quad <- test_factors %>% 
  mutate(
    prob_pobre_rf_weighted = predict(
      cv_rf_weighted, 
      newdata = test_factors[, setdiff(names(test_factors), "id")], 
      type = "prob"
    )[,"Yes"],

    pred_pobre_cat_rf_weighted = ifelse(prob_pobre_rf_weighted > umbral_gral, "Yes", "No"),
    pobre = ifelse(pred_pobre_cat_rf_weighted == "Yes", 1, 0)
  )

# Guardar resultados
resultados_prediccion_rf_weighted_quad <- predictTest_rf_weighted %>%
  select(id, pobre)
```

## 4.18 Guardar para kaggle
```{r}
## 4.14 Template_RF_CV_weighted
write.csv(
  resultados_prediccion_rf_weighted, 
  "C:/MECA/2025/BIG DATA Y MACHINE LEARNING- Ignasio Sarmiento/Taller2/T2_BDML/05_Predictions/100425sample_equipo4_rf_cv_wg_sq.csv", 
  row.names = FALSE
)

```


```{r}
# 1. Predecir probabilidades en el set de entrenamiento
prob_rf_train <- predict(cv_rf_weighted, newdata = train_importantes, type = "prob")[,"Yes"]
prob_rf_train


# 2. Clasificación con el umbral definido por ti
pred_rf_train <- ifelse(prob_rf_train > umbral_gral, "Yes", "No")

# 3. Matriz de confusión
confusionMatrix(
  factor(pred_rf_train), 
  factor(train_importantes$Pobre),
  positive = "Yes"
)

# 4. F1 Score
F1_Score(
  y_pred = pred_rf_train, 
  y_true = train_importantes$Pobre, 
  positive = "Yes"
)

# 5. AUC y curva ROC
roc_rf <- roc(response = train_importantes$Pobre, predictor = prob_rf_train, levels = c("No", "Yes"))
auc_rf <- auc(roc_rf)
print(paste("AUC:", round(auc_rf, 4)))

# 6. Graficar la curva ROC
plot(roc_rf, col = "darkgreen", main = "Curva ROC - Random Forest Weighted (Train)")
```
## 4.19 Evaluación de modelo
```{r}
precision <- Precision(y_pred = pred_rf_train, y_true = train_importantes$Pobre, positive = "Yes")
recall <- Recall(y_pred = pred_rf_train, y_true = train_importantes$Pobre, positive = "Yes")

print(paste("Precision:", round(precision, 4)))
print(paste("Recall:", round(recall, 4)))

```


# 5. Naive Bayes

## 5.1 Detección variables varianza cero

```{r}
p_load(caret)

# Detectar variables con varianza cercana a cero
casi_cero <- nearZeroVar(train_factors, saveMetrics = TRUE)

# Ver variables con varianza casi cero
casi_cero[casi_cero$nzv == TRUE, ]

# Crear versión filtrada solo para Naive Bayes
vars_quitar <- c("hipoteca", "sin_titulo", "H_afi_salud")
train_nb <- train_factors[, !(names(train_factors) %in% vars_quitar)]
test_nb <- test_factors[, !(names(test_factors) %in% vars_quitar)]

```

## 5.2 Grilla Naive Bayes

```{r}
p_load(naivebayes, caret)

# Definir grilla de hiperparámetros para Naive Bayes
grid_nb <- expand.grid(
  usekernel = c(TRUE, FALSE),
  adjust = c(0.5, 1, 2),
  laplace = c(0, 1)
)

```

## 5.3 Entrenamiento CV

```{r}
# Control de entrenamiento con validación cruzada
ctrl_nb <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  sampling = "up",
  verboseIter = FALSE
)

# Entrenar modelo Naive Bayes
set.seed(123)
nb_model <- train(
  Pobre ~ ., 
  data = train_nb,
  method = "naive_bayes",
  trControl = ctrl_nb,
  tuneGrid = grid_nb,
  metric = "ROC",
  preProcess = c("center", "scale")
)

# Ver resultados del modelo
nb_model

```

## 5.4 Predicciones 

```{r}
# Obtener probabilidades sobre el test set
pred_prob_nb <- predict(nb_model, newdata = test_nb, type = "prob")

#Umbral de 0.3
pred_clase_nb <- ifelse(pred_prob_nb[, "Yes"] >= 0.3, 1, 0)

# Crear dataframe final para enviar
predicciones_kaggle_nb <- data.frame(id = test$id, Pobre = pred_clase_nb)

# Ver tabla resumen
table(predicciones_kaggle_nb$Pobre)
```

## 5.5 Template Naive Bayes

```{r}
## Template_RF
write.csv(predicciones_kaggle_nb, "../05_Predictions/sample_equipo4_naivebayes.csv", 
          row.names = FALSE,
          quote = FALSE)
```


#6. Boosting / XGBoost

##6.1 Definir Grilla XGBoost

```{r}
p_load(xgboost)

grid_xgboost <- expand.grid(nrounds = c(250,500),
                            max_depth = c(1, 2),
                            eta = c(0.1,  0.01), 
                            gamma = c(0, 1), 
                            min_child_weight = c(10, 25),
                            colsample_bytree = c(0.4, 0.7), 
                            subsample = c(0.7))
grid_xgboost
```

##6.2 Balanceo de pesos

```{r}

# Calcular proporción de clases para asignar peso
weight_ratio <- sum(train_factors$Pobre == "No") / sum(train_factors$Pobre == "Yes")

# Crear vector de pesos
weights <- ifelse(train_factors$Pobre == "Yes", weight_ratio, 1)

# Control de entrenamiento con ROC
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,                   
  summaryFunction = twoClassSummary,  
  verboseIter = FALSE
)

```

##6.3 Entrenamiento 

```{r}
# Entrenar el modelo XGBoost con fórmula directa
set.seed(123)
xgb_model <- train(
  Pobre ~ .,                          
  data = train_factors,               
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = grid_xgboost,
  metric = "ROC",     
  weights = weights,                  
  verbosity = 0
)

print(xgb_model)

```

##6.4 Probabilidades XGBoost

```{r}
# Obtener probabilidades
pred_prob_xgboost <- predict(xgb_model, newdata = test_factors, type = "prob")
Pobre_binaria_xgboost <- ifelse(pred_prob_xgboost[, "Yes"] >= 0.5, 1, 0)
resultados_prediccion_xgboost_2 <- data.frame(id = test$id, Pobre = Pobre_binaria_xgboost)

table(resultados_prediccion_xgboost_2$Pobre)


```

##6.5 Template XGBoosting

```{r}
## Template_RF
write.csv(resultados_prediccion_xgboost_2,
          "../predicciones/06-04-25_equipo4_cart.csv", row.names = FALSE, 
          quote = FALSE)
```

# 7. Entrenamiento algoritmo CART

## 7.1 Entrenamiento del algoritmo CART- Cuando usamos rpart sin fijar un parámetro de complejidad cp el arbol es podado usando por α = 0,01 por defecto

```{r}
# Ver importancia de variables
importancia <- arbol$variable.importance
print(importancia)
```

```{r}
train_factors_model <- train_factors %>% select(-id)



arbol <- rpart(Pobre ~ Nper + nmujeres + ncotizando + maxEducLevel + nt_trab_semanal 
               + H_cotizando + nmayor_edad, 
                       data    = train_factors_model,
                       method = "class" ## define tree for classification
                       )
arbol$control$cp # alpha
```

```{r}
prp(arbol, under = TRUE, branch.lty = 2, yesno = 2, faclen = 0, varlen=15,box.palette = "-RdYlGn")
```

## 7.2 Predicción

```{r}
# variables categóricas del test tengan los mismos niveles y tipos
test_factors_model <- test %>% 
  mutate(Dominio=factor(Dominio),
         pagada=factor(pagada,levels = c(0,1),labels = c("No","Yes")),
         hipoteca=factor(hipoteca,levels = c(0,1),labels = c("No","Yes")),
         arriendo=factor(arriendo,levels = c(0,1),labels = c("No","Yes")),
         usufructo=factor(usufructo,levels = c(0,1),labels = c("No","Yes")),
         sin_titulo=factor(sin_titulo,levels = c(0,1),labels = c("No","Yes")),
         H_Head_Educ_level = factor(H_Head_Educ_level, levels = c(0:6), labels = c("Ns", "Ninguno", "Preescolar", "Primaria", "Secundaria", "Media", "Universitaria")),
         maxEducLevel = factor(maxEducLevel, levels = c(0:6), labels = c("Ns", "Ninguno", "Preescolar", "Primaria", "Secundaria", "Media", "Universitaria"))
  )

pred_prob_cart <- predict(arbol, newdata = test_factors_model, type = "prob")    ## Predecir la probabilidad (en lugar de la clase)
```

```{r}
# Para Kaggle
ids <- test$id

# Convertir probabilidades 
pred_binaria <- ifelse(pred_prob_cart[, "Yes"] > 0.5, 1, 0)


submission <- data.frame(id = ids, Pobre = pred_clase)
# 1. Revisa estructura
str(submission)
```

##7.2 Template_CART

```{r}
write.csv
write.csv(submission, 
          "../predicciones/06-04-25_equipo4_cart.csv", row.names = FALSE, 
          quote = FALSE)
```

# 8. Entrenamiento algoritmo de Regresión Lineal

## 8.1 Entrenamiento del modelo con CV y 5 folds

```{r}
set.seed(122)
# Control de entrenamiento sin classProbs, porque no es clasificación
ctrl_lm <- trainControl(method = "cv", number = 5)

# Entrenar modelo de regresión lineal
modelo_lm <- train(
  Pobre ~ ., 
  data = train, 
  method = "lm", 
  trControl = ctrl_lm
)

```

## 8.3 Predicción con valores continuos train

```{r}
# Predicciones 
pred_lm_train <- predict(modelo_lm, newdata = train)

# Convertir a clase usando umbral 0.5
pred_class_train_lm <- ifelse(pred_lm_train >= 0.5, 1, 0)

```

## 8.4 Matriz de confusión para regresión lineal

```{r}
# Matriz de confusión
confusionMatrix(factor(pred_class_train_lm), factor(train$Pobre))

# Calcular métrica F1
F1_Score(y_pred = pred_class_train_lm, y_true = train$Pobre, positive = "1")

```

## 8.5 Predicción

```{r}
# Predicción
pred_test_lm <- predict(modelo_lm, newdata = test)

# Clasificar con umbral 0.5
pred_binaria_test_lm <- ifelse(pred_test_lm >= 0.5, 1, 0)

```

## 8.6 Template regresión lineal

```{r}
# Crear archivo para submission
submission_lm <- data.frame(id = test$id, Pobre = pred_binaria_test_lm)

# 10. Exportar
write.csv(submission_lm, 
          "../05_Predictions/09-04-25_equipo4_RegLineal.csv", 
          row.names = FALSE, 
          quote = FALSE)
```

## 8.7 Selección de variables importantes

```{r}
# Importancia de las variables
importance_lm <- varImp(modelo_lm, scale = TRUE)
print(importance_lm)

importance_df_lm <- importance_lm$importance
importance_df_lm$Variable <- rownames(importance_df_lm)

# Seleccionar top 8
top_vars <- importance_df_lm[order(-importance_df_lm$Overall), ][1:8, ]

# Crear gráfico con ggplot2
graf_import_lm <- ggplot(top_vars, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "gray") +
  coord_flip() +
  labs(
    title = "Importancia de las variables - Modelo Regresión Lineal",
    x = "Variables",
    y = "Importancia escalada"
  ) +
  theme_bw()

# Guardar
ggsave("../04_Views/GRÁFICO-IMPORTANCIA DE VARIABLES REGRESIÓN LINEAL.png",
       plot = graf_import_lm, width = 10, height = 6, dpi = 300)

# Extraer nombres de las 8 variables más importantes
top_vars <- rownames(importance_lm$importance)[order(importance_lm$importance$Overall, decreasing = TRUE)][1:8]
print(top_vars)

# Obtener nombres de las 8 variables más importantes
vars_top <- rownames(importance_lm$importance)[order(importance_lm$importance$Overall, decreasing = TRUE)][1:8]

# Crear fórmula con solo esas variables
formula_top_ml <- as.formula(paste("Pobre ~", paste(vars_top, collapse = " + ")))
```

## 8.8 Entramiento del modelo con variables más importantes

```{r}
# Volver a entrenar modelo con variables seleccionadas
modelo_lm_top <- train(
  formula_top_ml,
  data = train,
  method = "lm",
  trControl = ctrl_lm
)

# Predicciones en train
pred_train_top_lm <- predict(modelo_lm_top, newdata = train)
pred_class_top_lm <- ifelse(pred_train_top_lm >= 0.5, 1, 0)
```

## 8.9 Nueva matriz de confusión y métrica

```{r}
# Matriz de confusión
confusionMatrix(factor(pred_class_top_lm), factor(train$Pobre))

# F1
F1_Score(y_pred = pred_class_top_lm, y_true = train$Pobre, positive = "1")
```
## 8.10 Predicción

```{r}
# Predicciones en test
pred_test_top_ml <- predict(modelo_lm_top, newdata = test)
pred_binaria_test_top_ml <- ifelse(pred_test_top_ml >= 0.5, 1, 0)
```

## 8.11 Template regresión lineal variales top

```{r}
# Crear nuevo archivo
submission_top_lm <- data.frame(id = test$id, Pobre = pred_binaria_test_top_ml)

# Exportar
write.csv(submission_top_lm, 
          "../05_Predictions/09-04-25_equipo4_RegLineal_TOP.csv", 
          row.names = FALSE, 
          quote = FALSE)
```

## 8.12 Modelo con interacción de las 8 variables top

```{r}
set.seed(122)
# 1. Control de entrenamiento (sin classProbs porque es regresión)
ctrl_lm_interac <- trainControl(method = "cv", number = 5)

# 2. Crear fórmula con interacciones de segundo orden
formula_interactiva <- as.formula(paste("Pobre ~ (", paste(top_vars, collapse = " + "), ")^2"))

# 3. Entrenamiento del modelo lineal con interacciones
modelo_interac <- train(
  formula_interactiva,
  data = train,
  method = "lm",
  trControl = ctrl_lm_interac
)

# 4. Predicciones en entrenamiento
pred_train <- predict(modelo_interac, newdata = train)

# 5. Convertir predicciones a clases usando umbral 
umbral <- 0.3
pred_clase <- ifelse(pred_train >= umbral, "Pobre", "NoPobre")

# 6. Convertir a factor para comparar
pred_clase <- factor(pred_clase, levels = c("Pobre", "NoPobre"))
actual_clase <- factor(ifelse(train$Pobre == 1, "Pobre", "NoPobre"), levels = c("Pobre", "NoPobre"))

# 7. Matriz de confusión
confusionMatrix(pred_clase, actual_clase)

# 8. F1 Score
F1_Score(y_pred = pred_clase, y_true = actual_clase, positive = "Pobre")

# 9. Predicciones en test
pred_test_interac_ml <- predict(modelo_interac, newdata = test)
pred_binaria_test_interac_ml <- ifelse(pred_test_interac_ml >= 0.5, 1, 0)

# 10. Crear nuevo archivo
submission_inteac_lm <- data.frame(id = test$id, Pobre = pred_binaria_test_interac_ml)

# Exportar
write.csv(submission_inteac_lm, 
          "../05_Predictions/09-04-25_equipo4_RegLineal_interac.csv", 
          row.names = FALSE, 
          quote = FALSE)
```

